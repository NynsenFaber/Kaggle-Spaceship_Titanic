{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91114b98-6a63-4486-9ec7-fabcdf4f0112",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:14:56.696636Z",
     "start_time": "2023-09-09T19:14:56.053835Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30665e7-8fcd-4d45-9eed-394f6ee5e8b1",
   "metadata": {},
   "source": [
    "# Import Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62f74f63-c491-4934-a0ae-3fc5b66d6786",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:14:56.743481Z",
     "start_time": "2023-09-09T19:14:56.699757Z"
    }
   },
   "outputs": [],
   "source": [
    "path = './spaceship-titanic/'\n",
    "data = pd.read_csv(path + 'train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9ffd8cd-e7ab-4524-a8bf-c2bf7bf4093b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:14:56.768296Z",
     "start_time": "2023-09-09T19:14:56.735138Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "  PassengerId HomePlanet CryoSleep  Cabin  Destination   Age    VIP  \\\n0     0001_01     Europa     False  B/0/P  TRAPPIST-1e  39.0  False   \n1     0002_01      Earth     False  F/0/S  TRAPPIST-1e  24.0  False   \n2     0003_01     Europa     False  A/0/S  TRAPPIST-1e  58.0   True   \n3     0003_02     Europa     False  A/0/S  TRAPPIST-1e  33.0  False   \n4     0004_01      Earth     False  F/1/S  TRAPPIST-1e  16.0  False   \n\n   RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  \\\n0          0.0        0.0           0.0     0.0     0.0    Maham Ofracculy   \n1        109.0        9.0          25.0   549.0    44.0       Juanna Vines   \n2         43.0     3576.0           0.0  6715.0    49.0      Altark Susent   \n3          0.0     1283.0         371.0  3329.0   193.0       Solam Susent   \n4        303.0       70.0         151.0   565.0     2.0  Willy Santantines   \n\n   Transported  \n0        False  \n1         True  \n2        False  \n3        False  \n4         True  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PassengerId</th>\n      <th>HomePlanet</th>\n      <th>CryoSleep</th>\n      <th>Cabin</th>\n      <th>Destination</th>\n      <th>Age</th>\n      <th>VIP</th>\n      <th>RoomService</th>\n      <th>FoodCourt</th>\n      <th>ShoppingMall</th>\n      <th>Spa</th>\n      <th>VRDeck</th>\n      <th>Name</th>\n      <th>Transported</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0001_01</td>\n      <td>Europa</td>\n      <td>False</td>\n      <td>B/0/P</td>\n      <td>TRAPPIST-1e</td>\n      <td>39.0</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Maham Ofracculy</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0002_01</td>\n      <td>Earth</td>\n      <td>False</td>\n      <td>F/0/S</td>\n      <td>TRAPPIST-1e</td>\n      <td>24.0</td>\n      <td>False</td>\n      <td>109.0</td>\n      <td>9.0</td>\n      <td>25.0</td>\n      <td>549.0</td>\n      <td>44.0</td>\n      <td>Juanna Vines</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0003_01</td>\n      <td>Europa</td>\n      <td>False</td>\n      <td>A/0/S</td>\n      <td>TRAPPIST-1e</td>\n      <td>58.0</td>\n      <td>True</td>\n      <td>43.0</td>\n      <td>3576.0</td>\n      <td>0.0</td>\n      <td>6715.0</td>\n      <td>49.0</td>\n      <td>Altark Susent</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0003_02</td>\n      <td>Europa</td>\n      <td>False</td>\n      <td>A/0/S</td>\n      <td>TRAPPIST-1e</td>\n      <td>33.0</td>\n      <td>False</td>\n      <td>0.0</td>\n      <td>1283.0</td>\n      <td>371.0</td>\n      <td>3329.0</td>\n      <td>193.0</td>\n      <td>Solam Susent</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0004_01</td>\n      <td>Earth</td>\n      <td>False</td>\n      <td>F/1/S</td>\n      <td>TRAPPIST-1e</td>\n      <td>16.0</td>\n      <td>False</td>\n      <td>303.0</td>\n      <td>70.0</td>\n      <td>151.0</td>\n      <td>565.0</td>\n      <td>2.0</td>\n      <td>Willy Santantines</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3117a88-1cbc-40de-bd91-c4ed171ecf1f",
   "metadata": {},
   "source": [
    "# Data-Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96cb59f9-aca4-49c1-953e-96cbdd380252",
   "metadata": {},
   "source": [
    "**PassengerId** - A unique Id for each passenger. Each Id takes the form gggg_pp where gggg indicates a group the passenger is travelling with and pp is their number within the group. People in a group are often family members, but not always.\n",
    "\n",
    "**HomePlanet** - The planet the passenger departed from, typically their planet of permanent residence.\n",
    "\n",
    "    We are going to use hot encoding here\n",
    "    Nan value is replace with the mode\n",
    "\n",
    "**CryoSleep** - Indicates whether the passenger elected to be put into suspended animation for the duration of the voyage. Passengers in cryosleep are confined to their cabins.\n",
    "\n",
    "    We are going to use binary encoding\n",
    "    Nan value is replace with the mode\n",
    "\n",
    "**Cabin** - The cabin number where the passenger is staying. Takes the form deck/num/side, where side can be either P for Port or S for Starboard.\n",
    "\n",
    "    We are going to use hot encoding\n",
    "    Nan value is replace with the mode\n",
    "    \n",
    "**Destination** - The planet the passenger will be debarking to.\n",
    "\n",
    "    We are going to use hot encoding\n",
    "    Nan value is replace with the mode\n",
    "\n",
    "**Age** - The age of the passenger.\n",
    "\n",
    "    Normalization MinMax\n",
    "    Nan value is replace with the average\n",
    "\n",
    "**VIP** - Whether the passenger has paid for special VIP service during the voyage.\n",
    "\n",
    "    Binary Encoding\n",
    "    Nan value is replace with the mode\n",
    "\n",
    "**RoomService, FoodCourt, ShoppingMall, Spa, VRDeck** - Amount the passenger has billed at each of the Spaceship Titanic's many luxury amenities.\n",
    "\n",
    "    Normalization MinMax\n",
    "    Nan values are replaced with the average\n",
    "\n",
    "**Name** - The first and last names of the passenger.\n",
    "\n",
    "    This feature is not necessary\n",
    "\n",
    "**Transported** - Whether the passenger was transported to another dimension. This is the target, the column you are trying to predict.\n",
    "\n",
    "    What we want to predict"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pre-process (Nan Values)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4efa7bf34c6e9e58"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler\n",
    "\n",
    "# fix NaN Value using the mode\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "column_to_impute = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP']\n",
    "data[column_to_impute] = imputer.fit_transform(data[column_to_impute])\n",
    "\n",
    "# remove Name\n",
    "data.drop(columns = ['Name', 'PassengerId', 'Cabin'], inplace=True)\n",
    "\n",
    "# fix NaN Value using the mode\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "column_to_impute = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP']\n",
    "data[column_to_impute] = imputer.fit_transform(data[column_to_impute])\n",
    "\n",
    "# fix numerical NaN value using the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "column_to_impute = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "data[column_to_impute] = imputer.fit_transform(data[column_to_impute])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T19:14:57.747716Z",
     "start_time": "2023-09-09T19:14:56.750659Z"
    }
   },
   "id": "334ebbcee2c371f7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pre-process (Encoding)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2e260ffc07ef4c3"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# econding categorical data\n",
    "hot_encoder_category = OneHotEncoder()\n",
    "# columns_to_encode = ['HomePlanet', 'Cabin', 'Destination'] # probably the Cabin is too much for HotEncoding\n",
    "columns_to_encode = ['HomePlanet', 'Destination']\n",
    "encoded_data = hot_encoder_category.fit_transform(data[columns_to_encode])\n",
    "encoded_df = pd.DataFrame(encoded_data.toarray(),\n",
    "                          columns=hot_encoder_category.get_feature_names_out(columns_to_encode),\n",
    "                          index = data.index)\n",
    "data.drop(columns = columns_to_encode, inplace=True)\n",
    "data = pd.concat([data, encoded_df], axis=1)\n",
    "\n",
    "# encoding binary data (True = 1, False = 0)\n",
    "columns_to_encode = ['CryoSleep', 'VIP']\n",
    "data[columns_to_encode] = data[columns_to_encode].astype(float)\n",
    "\n",
    "# normalized numerical data\n",
    "scaler = MinMaxScaler()\n",
    "columns_to_scale = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "scaled_data = scaler.fit_transform(data[columns_to_scale])\n",
    "\n",
    "scaled_data = pd.DataFrame(scaled_data,\n",
    "                            columns=columns_to_scale,\n",
    "                            index=data.index)\n",
    "\n",
    "for column in columns_to_scale:\n",
    "    data[column] = scaled_data[column]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T19:14:57.795796Z",
     "start_time": "2023-09-09T19:14:57.754341Z"
    }
   },
   "id": "f9b3f8e77019fb62"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create dataset for pytorch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "29baad6400f6dc7a"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b8f7618-c40f-4c57-8bac-20bf0c89d77c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:15:00.860471Z",
     "start_time": "2023-09-09T19:14:57.774697Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# define the Dataset class to be used for pytorch training\n",
    "\n",
    "class Data(Dataset):\n",
    "\n",
    "    def __init__(self, \n",
    "                 df: pd.DataFrame,\n",
    "                 device: torch.device = torch.device('cpu')):\n",
    "        self.device = device\n",
    "    \n",
    "        self.X = df.iloc[:, :-1]\n",
    "        self.Y = df.iloc[:, [-1]].astype(int) # True = 1, False = 0\n",
    "    \n",
    "    def __getitem__(self, i:int) -> (torch.tensor, torch.tensor):\n",
    "        X = self.X.iloc[i].values.astype(np.float32)\n",
    "        X = torch.tensor(X, device=self.device)\n",
    "        Y = self.Y.iloc[i].values.astype(np.float32)\n",
    "        Y = torch.tensor(Y, device=self.device)\n",
    "        # change dtype to torch.float32 (same for the labels)\n",
    "        return X, Y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split the data into train and test set"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f21aa0a9767df4ff"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "# shuffle the index of the dataset\n",
    "idx = np.random.permutation(data.index)\n",
    "# split the dataset into train and test set\n",
    "train_idx = idx[:int(len(idx) * 0.8)]\n",
    "test_idx = idx[int(len(idx) * 0.8):]\n",
    "# create the train and test set\n",
    "train_data = data.iloc[train_idx].reset_index(drop=True)\n",
    "test_data = data.iloc[test_idx].reset_index(drop=True)\n",
    "# create the dataset for pytorch\n",
    "# set the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "train_dataset = Data(train_data, device=device)\n",
    "test_dataset = Data(test_data, device=device)\n",
    "# create the dataloader for pytorch\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T19:15:00.871748Z",
     "start_time": "2023-09-09T19:15:00.863189Z"
    }
   },
   "id": "589bf7fc1404c9e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Define the neural network"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f06e3b25b466f6f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We create a simple neural network with 3 layers, the first one with 100 neurons, the second one with 50 neurons and the last one with 1 neuron. The first two layers use the LeakyReLU activation function, while the last one uses the Sigmoid activation function."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1064b0834194b4f0"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcbaf1ae-7a88-41b5-b5b0-aabe73d3989b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-09T19:15:00.893977Z",
     "start_time": "2023-09-09T19:15:00.871905Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BinaryClassificationNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size=100):\n",
    "        super(BinaryClassificationNN, self).__init__()\n",
    "\n",
    "        # Define the layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.LeakyReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.relu2 = nn.LeakyReLU()\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Forward pass through the network\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.sigmoid(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the neural network"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83067c5d26bf85b6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialize the neural network"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7fcdd58a1dd29a65"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "dim_input = train_dataset[0][0].shape[0]\n",
    "# Create an instance of the binary classification neural network\n",
    "model = BinaryClassificationNN(input_size=dim_input)\n",
    "model.to(device)\n",
    "\n",
    "# Define the loss function (usually Binary Cross-Entropy for binary classification)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Define the optimizer (usually Adam or SGD)\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T19:15:00.902189Z",
     "start_time": "2023-09-09T19:15:00.880330Z"
    }
   },
   "id": "25a58cf5eb9f6271"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define the train function"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bcbfa75de48338f0"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def train(model, dataloader, criterion, optimizer, num_epochs, seed=42):\n",
    "    \"\"\"\n",
    "    Train a PyTorch model for binary classification.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to train.\n",
    "        dataloader (DataLoader): DataLoader for your training dataset.\n",
    "        criterion (nn.Module): The loss function (e.g., BCELoss for binary classification).\n",
    "        optimizer (torch.optim.Optimizer): The optimizer (e.g., Adam, SGD).\n",
    "        num_epochs (int): The number of training epochs.\n",
    "        seed (int): The random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        None (modifies the model in-place).\n",
    "    \"\"\"\n",
    "    model.train()  # Set the model to training mode\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in dataloader:\n",
    "            # to get the same learning result\n",
    "            torch.manual_seed(seed + epoch) \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Compute the loss\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backpropagation and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the running loss\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Print the average loss for this epoch\n",
    "        epoch_loss = running_loss / len(dataloader)\n",
    "        print(f\"Epoch [{epoch + 1}/{num_epochs}] Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    print(\"Training complete!\")\n",
    "\n",
    "# Usage example:\n",
    "# train(model, train_dataloader, criterion, optimizer, device, num_epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T19:15:00.902345Z",
     "start_time": "2023-09-09T19:15:00.889294Z"
    }
   },
   "id": "4a6b0d5aa163e3c1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b8902cdd6696c3b0"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] Loss: 0.2078\n",
      "Epoch [2/10] Loss: 0.0012\n",
      "Epoch [3/10] Loss: 0.0003\n",
      "Epoch [4/10] Loss: 0.0001\n",
      "Epoch [5/10] Loss: 0.0001\n",
      "Epoch [6/10] Loss: 0.0000\n",
      "Epoch [7/10] Loss: 0.0000\n",
      "Epoch [8/10] Loss: 0.0000\n",
      "Epoch [9/10] Loss: 0.0000\n",
      "Epoch [10/10] Loss: 0.0000\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "train(model, train_loader, criterion, optimizer, num_epochs, seed=seed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T19:15:52.558Z",
     "start_time": "2023-09-09T19:15:00.893216Z"
    }
   },
   "id": "4693ffccd032efb0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate the model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4044bf8c9655b61"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def evaluate(model, dataloader, criterion):\n",
    "    \"\"\"\n",
    "    Evaluate a PyTorch model for binary classification.\n",
    "\n",
    "    Args:\n",
    "        model (nn.Module): The PyTorch model to evaluate.\n",
    "        dataloader (DataLoader): DataLoader for your evaluation dataset.\n",
    "        criterion (nn.Module): The loss function (e.g., BCELoss for binary classification).\n",
    "        device (torch.device): The device on which to run the evaluation.\n",
    "\n",
    "    Returns:\n",
    "        float: The average loss over the evaluation dataset.\n",
    "        float: The accuracy over the evaluation dataset.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():  # Deactivate gradients for the following code\n",
    "        for inputs, labels in dataloader:\n",
    "\n",
    "            # Forward pass and loss computation\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Convert the probabilities to integers (0 or 1)\n",
    "            predicted_classes = (outputs >= 0.5).int().float()\n",
    "\n",
    "            # Count the correct predictions\n",
    "            correct_predictions += (predicted_classes == labels).sum().item()\n",
    "            total_predictions += len(labels)\n",
    "\n",
    "    # Compute the loss and accuracy\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    acc = correct_predictions / total_predictions\n",
    "\n",
    "    print(f\"Loss: {avg_loss:.4f}, Accuracy: {acc:.4f}\")\n",
    "\n",
    "    return avg_loss, acc"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T19:15:52.558199Z",
     "start_time": "2023-09-09T19:15:11.203718Z"
    }
   },
   "id": "ca310dccd94e15a"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.0000, Accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": "(1.0121378434989997e-05, 1.0)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, test_loader, criterion)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T19:15:52.558430Z",
     "start_time": "2023-09-09T19:15:11.207991Z"
    }
   },
   "id": "eef4c67ef826b244"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluate on the submition test dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d3576aac67650753"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(path + 'test.csv')\n",
    "passenger_id = test_data['PassengerId']"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T19:15:52.558474Z",
     "start_time": "2023-09-09T19:15:11.646751Z"
    }
   },
   "id": "a11ff43913c2f100"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pre-process (Nan Values)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "91b1e449650d8fdb"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# fix NaN Value using the mode\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "column_to_impute = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP']\n",
    "test_data[column_to_impute] = imputer.fit_transform(test_data[column_to_impute])\n",
    "\n",
    "# remove Name\n",
    "test_data.drop(columns = ['Name', 'PassengerId', 'Cabin'], inplace=True)\n",
    "\n",
    "# fix NaN Value using the mode\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "column_to_impute = ['HomePlanet', 'CryoSleep', 'Destination', 'VIP']\n",
    "test_data[column_to_impute] = imputer.fit_transform(test_data[column_to_impute])\n",
    "\n",
    "# fix numerical NaN value using the mean\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "column_to_impute = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "test_data[column_to_impute] = imputer.fit_transform(test_data[column_to_impute])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T19:15:52.558516Z",
     "start_time": "2023-09-09T19:15:11.664755Z"
    }
   },
   "id": "196887fd2fba25cf"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pre-process (Encoding)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2bf9cb1af1ddb796"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use the same encoded used for the train dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "52745aa8925e6b52"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# econding categorical data\n",
    "columns_to_encode = ['HomePlanet', 'Destination'] # probably the Cabin is too much for HotEncoding\n",
    "encoded_data = hot_encoder_category.transform(test_data[columns_to_encode])\n",
    "encoded_df = pd.DataFrame(encoded_data.toarray(),\n",
    "                          columns=hot_encoder_category.get_feature_names_out(columns_to_encode),\n",
    "                          index = test_data.index)\n",
    "\n",
    "test_data.drop(columns = columns_to_encode, inplace=True)\n",
    "test_data = pd.concat([test_data, encoded_df], axis=1)\n",
    "\n",
    "# encoding binary data (True = 1, False = 0)\n",
    "columns_to_encode = ['CryoSleep', 'VIP']\n",
    "test_data[columns_to_encode] = test_data[columns_to_encode].astype(float)\n",
    "\n",
    "# normalized numerical data\n",
    "columns_to_scale = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "scaled_data = scaler.transform(test_data[columns_to_scale])\n",
    "\n",
    "scaled_data = pd.DataFrame(scaled_data,\n",
    "                           columns=columns_to_scale,\n",
    "                           index=test_data.index)\n",
    "\n",
    "for column in columns_to_scale:\n",
    "    test_data[column] = scaled_data[column]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T19:15:52.558561Z",
     "start_time": "2023-09-09T19:15:11.685272Z"
    }
   },
   "id": "29216b21f8f871ea"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create dataset for pytorch"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ccd21edb3e74add6"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "class PredictData(Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 df: pd.DataFrame,\n",
    "                 device: torch.device = torch.device('cpu')):\n",
    "        self.device = device\n",
    "\n",
    "        self.X = df.iloc[:, :]\n",
    "\n",
    "    def __getitem__(self, i:int) -> (torch.tensor, torch.tensor):\n",
    "        X = self.X.iloc[i].values.astype(np.float32)\n",
    "        X = torch.tensor(X, device=self.device)\n",
    "        # change dtype to torch.float32 (same for the labels)\n",
    "        return X\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T19:15:52.558602Z",
     "start_time": "2023-09-09T19:15:11.695707Z"
    }
   },
   "id": "ed5a8967bc945610"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "test_dataset = PredictData(test_data, device=device)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T19:15:52.558644Z",
     "start_time": "2023-09-09T19:15:11.699423Z"
    }
   },
   "id": "a29574d215a66116"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create dataset to send"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f8bb0bc0f6fbdc5f"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def create_dataset_prediction(model, dataloader, indices):\n",
    "    \n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions = {}\n",
    "    with torch.no_grad():  # Deactivate gradients for the following code\n",
    "        for inputs, id in zip(dataloader, indices):\n",
    "            inputs = inputs[0]\n",
    "            # Forward pass and loss computation\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            # Convert the probabilities to integers (0 or 1)\n",
    "            predicted_classes = (outputs >= 0.5).int()\n",
    "            \n",
    "            predictions[id] = bool(predicted_classes)\n",
    "\n",
    "    return predictions"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T19:15:52.558684Z",
     "start_time": "2023-09-09T19:15:11.704781Z"
    }
   },
   "id": "3587537b5a2241f"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "predictions = create_dataset_prediction(model, test_loader, passenger_id)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T19:15:52.558724Z",
     "start_time": "2023-09-09T19:15:11.707875Z"
    }
   },
   "id": "8747adfe5284a87c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create the submission file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7d7be35cd417db86"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "predictions_df = pd.DataFrame(predictions.items(), columns=['PassengerId', 'Transported'])\n",
    "predictions_df.set_index('PassengerId', inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T19:15:52.558765Z",
     "start_time": "2023-09-09T19:15:12.453183Z"
    }
   },
   "id": "3eeb5bd5f778153a"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "             Transported\nPassengerId             \n0013_01            False\n0018_01            False\n0019_01            False\n0021_01             True\n0023_01             True\n...                  ...\n9266_02            False\n9269_01             True\n9271_01            False\n9273_01            False\n9277_01            False\n\n[4277 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Transported</th>\n    </tr>\n    <tr>\n      <th>PassengerId</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0013_01</th>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>0018_01</th>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>0019_01</th>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>0021_01</th>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>0023_01</th>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9266_02</th>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>9269_01</th>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>9271_01</th>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>9273_01</th>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>9277_01</th>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>4277 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T19:15:52.559221Z",
     "start_time": "2023-09-09T19:15:12.460364Z"
    }
   },
   "id": "df8f8606ffd4afa6"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Save the submission file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a23b0110a71a8d8"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "predictions_df.to_csv('submission.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T19:15:52.559269Z",
     "start_time": "2023-09-09T19:15:12.464769Z"
    }
   },
   "id": "f203a9014d234941"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T19:15:52.559315Z",
     "start_time": "2023-09-09T19:15:12.470508Z"
    }
   },
   "id": "2a565ed61eeb0bf4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
